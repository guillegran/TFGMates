\chapter{Preliminares de geometría diferencial}
\section{Derivada de Lie y fórmulas de Cartan}\label{sec:lie}

Antes de comenzar con el estudio de la geometría simpléctica, conviene recordar el concepto de la derivada de Lie de campos, extenderlo a formas y obtener una serie de resultados que nos serán útiles más adelante.

Empecemos recordando la definición del corchete de Lie de campos; su estudio detallado puede encontrarse en \cite{variedades}.

\begin{defn}
  \em
  Sea $M$ una variedad diferenciable y $\mathfrak{X}(M)$ el conjunto de los campos diferenciables en $M$. Se define el \emph{corchete de Lie} como la aplicación
  \begin{equation*}
    \begin{array}{rcl}
    \lie{\ }{\ }: \mathfrak{X}(M) \times \mathfrak{X}(M) & \longrightarrow & \mathfrak{X}(M) \\
    (X,Y) & \longmapsto & \lie{X}{Y} = X \circ Y - Y \circ X,
  \end{array}
  \end{equation*}
  donde la composición se entiende si vemos los campos como aplicaciones $\mathscr{C}^{\infty}(M) \rightarrow \mathscr{C}^{\infty}(M)$.
\end{defn}
\begin{figure}[h]
  \centering
  \includegraphics{pics/lie}
  \caption{\small Visión geométrica de la derivada de Lie}
  \label{fig:lie}
\end{figure}
Recordamos también que podemos ver el corchete de Lie de otra forma equivalente. Sean $M$ una variedad diferenciable, $a \in M$, $Y \in \mathfrak{X}(M)$, $\varphi$ un flujo en $M$ y $X$ su generador infinitesimal. Entonces
\begin{equation*}
  \lie{X}{Y}_a= \lim_{t\rightarrow 0}\frac{\varphi_{-t,*}(Y_{\varphi_t(a)})-Y_a}{t}.
\end{equation*}
En vista de esta fórmula, se define la \emph{derivada de Lie de Y respecto de X} como $L_XY=\lie{X}{Y}$. 
Por último, recordamos otro resultado muy importante que usaremos posteriormente:
\begin{prop}
  Sea $M$ una variedad diferenciable, sean $\varphi$, $\psi$ flujos en $M$ y sean $X$, $Y$ sus generadores infinitesimales, respectivamente. Entonces los flujos conmutan si y sólo si lo hacen sus generadores infinitesimales (es decir, $\varphi_t \circ \psi_s = \psi_s \circ \varphi_t$ si y sólo si $\lie{X}{Y}=0$).
\end{prop} 

Ya estamos en disposición de dar una definición más general de la derivada de Lie:
\begin{defn}
  \em
  Sean $M$ una variedad diferenciable, $X$ un campo en $M$, y $\varphi$ su flujo. Se define la \emph{derivada de Lie respecto de X} como la aplicación  \begin{equation*}
    \begin{array}{rcl}
    L_X: \Gamma^r(M)& \longrightarrow & \Gamma^r (M) \\
    \omega & \longmapsto & L_X \omega = \lim_{t\rightarrow 0}\frac{\varphi^*_t \omega - \omega}{t}
  \end{array}
  \end{equation*}
  (es decir, $(L_X\omega)_x=\lim_{t\rightarrow 0}\frac{\varphi^*_t\omega_{\varphi_t(x)}-\omega_x}{t}$ para $x \in M$).
\end{defn}

Vamos a obtener ahora un par de propiedades de la derivada de Lie.

\begin{prop}
  Sean $M$ una variedad diferenciable, $\omega \in \Gamma^r(M)$, $X,X_1,\dots,X_r\in \mathfrak{X} (M)$. Se cumple
  \begin{equation*}
    L_{X}\omega(X_1,\dots,X_r)=X\omega(X_1,\dots,X_r)-\sum_{i=1}^r \omega(X_1,\dots,\lie{X}{X_i},\dots,X_r).
  \end{equation*}
\end{prop}
\begin{proof}
  Vamos a probarlo sólo para el caso en el que $\omega$ es una 2-forma para simplificar su lectura. El cálculo general es completamente análogo.
  En primer lugar,
  \begin{align*}
    \lim_{t\rightarrow 0}\frac{1}{t}[(\varphi_t^* \omega)(X_1,X_2) - \omega (X_1,X_2)] =&\lim_{t\rightarrow 0}\left[\frac{1}{t}[(\varphi_t^* \omega)(X_1,X_2) - \varphi_t^*(\omega(X_1,X_2))]\right] \\
    &+ \lim_{t\rightarrow 0}\left[\frac{1}{t}[\varphi_t^*(\omega(X_1,X_2))-\omega(X_1,X_2)]\right].
  \end{align*}

  El segundo término de esta expresión es exactamente
  \begin{equation*}
    \lim_{t\rightarrow 0}\left[\frac{1}{t}[\varphi_t^*(\omega(X_1,X_2))-\omega(X_1,X_2)]\right]=\left( \left. \frac{d}{dt}\right|_{t=0} \varphi_t \right)(\omega(X_1,X_2))=X\omega(X_1,X_2),
  \end{equation*}
  mientras que el primer término, en $x\in M$ es
  \begin{align*}
     & \left.\lim_{t\rightarrow 0}\left[\frac{1}{t}[(\varphi_t^* \omega)(X_1,X_2) - \varphi_t^*(\omega(X_1,X_2))]\right]\right|_{x}  \\
     & = \lim_{t \rightarrow 0}\frac{1}{t}[\omega_{\varphi_t(x)}(d_x \varphi_t(X_{1,x}),d_x \varphi_t(X_{2,x})) -\omega_{\varphi_t(x)}(X_{1,\varphi_t(x)},X_{2,\varphi_t(x)})] \\
     & = \lim_{t\rightarrow 0} \omega_{\varphi_t(x)}\left[\frac{1}{t}(d_x \varphi_t(X_{1,x})-X_{1,\varphi_t(x)}),d_x\varphi_t(X_{2,x})\right] \\\ &+ \lim_{t\rightarrow 0} \omega_{\varphi_t(x)}\left[ X_{1,\varphi_t (x)}, \frac{1}{t}(d_x \varphi_t (X_{2,x})-X_{2,\varphi_t(x)}) \right] \\
     & = -\omega_x(\lie{X}{X_1}_x,X_{2,x})-\omega_x(X_{1,x},\lie{X}{X_2}_x).
  \end{align*}
  Comprobemos que, en efecto
  \begin{equation*}
    \lim_{t\rightarrow 0}\tfrac{1}{t}(d_x\varphi_t(X_{1,x})-X_{1,\varphi_t(x)})=\lie{X}{X_1}_x,
  \end{equation*}
  y es análogo para $\lie{X}{X_2}_x$.
  Basta «sacar factor común» a $d_x\varphi_t$, de modo que
  \begin{equation*}
    \tfrac{1}{t}(d_x\varphi_t(X_{1,x})-X_{1,\varphi_t(x)})=-d_x\varphi_t\left( \frac{\left( d_x\varphi_t \right)^{-1}\left( X_{1,\varphi_t(x)} \right)-X_{1,x}}{t} \right).
  \end{equation*}
  Ahora, $(d_x\varphi_t)^{-1}=d_{\varphi_t(x)}\varphi_{-t}=\varphi_{-t,*}$ y 
  \begin{equation*}
    \lim_{t\rightarrow 0} \tfrac{1}{t}(\varphi_{-t,*}(X_{1,\varphi_t(x)})-X_{1,x})=\lie{X}{X_1}_x.
  \end{equation*}

  Volviendo a agrupar, tenemos lo que se quería demostrar.
\end{proof}
\begin{prop}
  Sea $M$ una variedad diferenciable y $\alpha \in \Gamma^r(M)$. Se cumple
  \begin{align*}
    (\dd \alpha)(X_1,\dots,X_{r+1})=\sum_{i=1}^{r+1}(-1)^{i-1}X_i \alpha (X_1,\dots,\hat{X_i},\dots,X_{r+1}) \\
    + \sum_{i<j}(-1)^{i+j}\alpha(\lie{X_i}{X_j},X_1,\dots,\hat{X_i},\dots,\hat{X_j},\dots,X_{r+1}),
  \end{align*}
  donde el circunflejo en un campo quiere decir que este campo se omite.
\end{prop}
\begin{proof}
En este caso probaremos solo la identidad más sencilla 
\begin{equation*}
  \dd \alpha (X,Y) = X\alpha(Y) - Y \alpha(X) - \alpha(\lie{X}{Y}),
\end{equation*}
válida para el caso en el que $\alpha$ es de grado 1. El caso general es completamente análogo.

En primer lugar, escribimos todo en coordenadas locales:
\begin{align*}
  \alpha &= \sum_i \alpha_i \dd \xx _i, \ \dd \alpha = \sum_{i,j} \dd \alpha_i \wedge \dd \xx _j = \sum_{i,j}\parcial{\alpha_i}{\xx_j} \dd \xx_i \wedge \xx_j, \\
  X &= \sum_i X_i \deriv{\xx_i}, \ 
  \lie{X}{Y} = \sum_{i,j} \left( X_i \parcial{Y_j}{\xx_i} - Y_i \parcial{X_j}{\xx_i} \right) \deriv{\xx_j}.
\end{align*}

Ahora, operando en estas coordenadas:
\begin{align*}
  \alpha(X) &= \sum_i \alpha_i X_i, \ 
  \dd \alpha (X,Y) = \sum_{i,j} \parcial{\alpha_i}{\xx_j} (X_jY_i-X_iY_j), \\
  \alpha(\lie{X}{Y}) &= \sum_{i,j} \left( X_i \parcial{Y_j}{\xx_i} - Y_i \parcial{X_j}{\xx_i} \right) \alpha_j = \sum_{i,j} \alpha_i X_j \parcial{Y_i}{\xx_j} - \sum_{i,j}\alpha_i Y_j \parcial{X_i}{\xx_j}, \\
  X(\alpha Y) &= \sum_{i,j}\alpha_i X_j \parcial{Y_i}{\xx_j} + Y_i X_j \parcial{\alpha_i}{\xx_j}, \ 
Y(\alpha X) = \sum_{i,j}\alpha_i Y_j \parcial{X_i}{\xx_j} + X_i Y_j \parcial{\alpha_i}{\xx_j}.
\end{align*}

Obtenemos entonces
\begin{equation*}
  X(\alpha(Y))-Y\alpha(X)-\alpha(\lie{X}{Y}) = \sum_{i,j} Y_iX_j \parcial{\alpha_i}{\xx_j} - X_iY_j \parcial{\alpha_i}{\xx_j} = \dd \alpha (X,Y).
\end{equation*}
\end{proof}

Antes de seguir, vamos a introducir una nueva operación para formas:
\begin{defn}
  \em
  Sea $M$ una variedad diferenciable y $X$ un campo en $M$. Se define el \emph{producto interior} o \emph{contracción} $i_X:\Gamma^{r+1}(M)\rightarrow \Gamma^r(M)$ por 
  \begin{equation*}
    i_X \omega(X_1,\dots,X_r)=\omega(X,X_1,\dots,X_r),
  \end{equation*}
  para $X_1,\dots,X_r \in \mathfrak{X}(M)$.
\end{defn}

Podemos probar ya una serie de fórmulas, debidas a Élie Cartan\footnote{En la literatura, la segunda de estas fórmulas suele llamarse «fórmula mágica de Cartan».}, que nos serán de gran utilidad posteriormente.
\begin{thm}[Fórmulas de Cartan]
  Sea $X$ un campo en una variedad diferenciable $M$, y consideramos la derivada de Lie $L_X$, el producto interior $i_X$, y la diferencial exterior $\dd$. Se cumplen las siguientes fórmulas:
  \begin{enumerate}
    \item[$1$.] $i_{\lie{X}{Y}}=L_X i_Y - i_Y L_X$, para todo $Y \in \mathfrak{X}(M)$,
    \item[$2$.] $L_X= \dd \circ i_X + i_X \circ \dd $,
    \item[$3$.] $L_X\circ \dd = \dd \circ L_X$.
  \end{enumerate}
\end{thm}
\begin{proof}\leavevmode

    $1$. Si $X_1,\dots,X_r \in \mathfrak{X}(M)$, entonces
      \begin{equation*}
	L_X[(i_Y \omega) (X_1,\dots,X_r)] = X \omega(Y,X_1,\dots,X_r)-\sum_{i=1}^r \omega(Y,X_1,\dots,\lie{X}{X_i},\dots,X_r), 
      \end{equation*}
      \begin{align*}
	i_Y[(L_X \omega) (X_1,\dots,X_r)] =&  X \omega(Y,X_1,\dots,X_r) \\ 
	& -\sum_{i=1}^r \omega(Y,X_1,\dots,\lie{X}{X_i},\dots,X_r) \\
	&- \omega(\lie{X}{Y},X_1,\dots,X_r).
      \end{align*}
      Por tanto
      \begin{align*}
	i_{\lie{X}{Y}}\omega (X_1,\dots,X_r) =&\omega(\lie{X}{Y},X_1,\dots,X_r) \\
	=& L_X[(i_Y \omega) (X_1,\dots,X_r)] \\
	&-i_Y[(L_X \omega) (X_1,\dots,X_r)].
      \end{align*}

    $2$. Usando la relación entre el corchete de Lie y la diferencial exterior que obtuvimos antes, tenemos
      \begin{align*}
	(\dd (i_X \alpha)) (X_1,\dots,X_r) =& \sum_{i}(-1)^{i-1}X_i \alpha(X,X_1,\dots,\hat{X_i},\dots,X_r) \\
	& + \sum_{i<j} (-1)^{i+j} \alpha (X,\lie{X_i}{X_j},X_1,\dots,\hat{X_i},\dots,\hat{X_j},\dots,X_r),
      \end{align*}
      \begin{align*}
	(i_X (\dd \alpha)) (X_1,\dots,X_r) =& \sum_{i}(-1)^{i}X_i \alpha(X,X_1,\dots,\hat{X_i},\dots,X_r) \\
	& + \sum_{i<j} (-1)^{i+j+1} \alpha (X,\lie{X_i}{X_j},X_1,\dots,\hat{X_i},\dots,\hat{X_j},\dots,X_r) \\
	& + X\alpha(X_1,\dots,X_r) + \sum_j (-1)^{j} \alpha (\lie{X}{X_j},X_1,\dots,\hat{X_j},\dots,X_r).
      \end{align*}
      Sumando ambas expresiones obtenemos
      \begin{align*}
	(\dd i_X \alpha +i_X \dd \alpha) (X_1,\dots,X_r) = & X\alpha(X_1,\dots,X_r) \\
	& + \sum_j (-1)^{j} (-1)^{j-1} \alpha (X_1,\dots,\lie{X}{X_j},\dots,X_r) \\
	= & X\alpha(X_1,\dots,X_r) - \sum_j \alpha (X_1,\dots,\lie{X}{X_j},\dots,X_r). 
      \end{align*}
    $3$. Utilizando ($2$) y que $\dd \circ \dd=0$, obtenemos
      \begin{align*}
	L_X \circ \dd &= (i_X \circ \dd) \circ \dd + (\dd \circ i_X) \circ \dd = \dd \circ i_X \circ \dd, \\
	\dd \circ L_X &= \dd \circ (i_X \circ \dd) + \dd \circ (\dd \circ i_X) = \dd \circ i_X \circ \dd,
      \end{align*}
      luego $L_X \circ \dd= \dd \circ L_X$.
\end{proof}

\section{Campos y formas dependientes del tiempo} \label{sec:tiempo}
Hasta ahora hemos tratado con campos tangentes a una variedad diferenciable, que de forma natural nos dan ecuaciones diferenciales \emph{autónomas} sobre esta variedad. Sin embargo, en algún uso posterior, especialmente en la demostración del teorema de Darboux (teorema \ref{tdarboux}), vamos a necesitar tratar ecuaciones \emph{no} autónomas, es decir, ecuaciones diferenciales que dependan de un parámetro extra, generalmente el tiempo. Para ello, es necesario introducir los campos y las formas dependientes del tiempo.

  \begin{defn} \leavevmode
    \em 
    Sea $ J\subset \RR $ un intervalo abierto y $ M $ una variedad diferenciable:
    \begin{enumerate}
      \item Un \emph{campo tangente (diferenciable) dependiente del tiempo} de $M$ es una aplicación \[\begin{array}{rcl}X:J\times M & \longrightarrow & TM \\ (t,x) & \longmapsto & (x,X_{t,x}) \end{array} \] que es diferenciable como aplicación entre variedades. 
      \item Una \emph{forma diferencial (diferenciable) de grado $r$ dependiente del tiempo} de $M$ es una aplicación
	\[\begin{array}{rcl} \alpha: J \times M & \longrightarrow & \varLambda^r(M) \\ (t,x) &  \longmapsto & (x,\alpha_{t,x}) \end{array}\] tal que la función \[\begin{array}{rcl} \alpha(X_1,\dots,X_r):J\times M & \longrightarrow & \RR \\ (t,x) & \longmapsto & \alpha_{t,x}(X_{1,x}^t,\dots,X_{r,x}^t)\end{array}\]
	es diferenciable para cualesquiera $r$ campos $X_1,\dots,X_r \in \mathfrak{X}(M)$.
    \end{enumerate}
  \end{defn}

  \begin{obs}
    \em
    Un campo y una forma dependientes del tiempo se expresan en una carta $(U,\xx)$ en la forma
    \begin{align*}
      X_{t,x} &= \sum X_i(t,x) \left.\deriv{\xx_i}\right|_x \\
      \alpha_{t,x} &= \sum \alpha_i(t,x) \dd \xx_i |_x.
    \end{align*}
    La diferenciabilidad de $X$ y $\alpha$ es equivalente a la de sus componentes $X_i$, $\alpha_i$ como funciones $J\times U \rightarrow \RR$.
  \end{obs}
  Tiene sentido decir ahora qué entendemos por derivar una forma respecto al tiempo. Sea $\alpha_t$ una $r$-forma dependiente del tiempo, la \emph{derivada temporal} de $\alpha_t$ es
    \begin{equation*}
      \left. \frac{d}{dt}\right|_{t=t_0} \alpha_t = \lim_{h\rightarrow 0}\frac{\alpha_{t_0+h}-\alpha_{t_0}}{h}.
    \end{equation*}

    El siguiente teorema nos permite integrar campos dependientes del tiempo. Por brevedad de exposición omitimos la demostración, que puede encontrarse en \cite{lee}.
  \begin{thm}
    Sea $X:J\times M \rightarrow TM$ un campo dependiente del tiempo. Existen un abierto $V\subset J\times J\times M$ y una función $\varphi:V\rightarrow M$ tal que para cada $s\in J$ y para cada $x\in M$, el conjunto $V^{(s,x)}=\left\{ t\in J | (t,s,x) \in V \right\}$ es un intervalo abierto que contiene a $s$ y la curva $\gamma:V^{(s,x)}\rightarrow M$ definida por $\gamma(t)=\varphi(t,s,x)$ es la única solución maximal al problema de valor inicial
    \begin{align*}
      \begin{cases}
      \gamma'(t)=X_{(t,\gamma(t))}, \\
      \gamma(s)=x.
    \end{cases}
    \end{align*}
  \end{thm}
    Equivalentemente, podemos ver el problema de valor inicial en la siguiente forma:
    \begin{align*}
      \begin{cases}
      \parcial{\varphi}{t}(t,s,x)=X_{(t,\varphi(t,s,x))}, \\
      \varphi(s,s,x)=x.
    \end{cases}
    \end{align*}
    Esta $\varphi$ recibe el nombre de \emph{flujo dependiente del tiempo}. Nótese que cada $\varphi(t,s,\bullet)$ es una aplicación diferenciable de un abierto de $M$ en $M$ que denotaremos $\varphi_{t,s}$. Aunque aquí no lo detallemos, es sencillo probar que este flujo sigue una \emph{ley de aditividad}: si $(t_1,t_0,x)\in V$ y $(t_2,t_1,\varphi_{t_1,t_0}(x))\in V$, entonces $(t_2,t_0,x)\in V$ y
    \begin{equation*}
      \varphi_{t_2,t_1}\circ \varphi_{t_1,t_0}(x)=\varphi_{t_2,t_0}(x).
    \end{equation*}

  Ahora podemos generalizar la derivada de Lie de formas para campos dependientes del tiempo:
  \begin{equation*}
    L_{X_{t}}\alpha=\lim_{h\rightarrow 0}\frac{\varphi^*_{t+h,t} \alpha - \alpha}{h},
  \end{equation*}
  para $x\in M$.
  Esto nos da una fórmula que nos relaciona la derivada temporal con la derivada de Lie de formas.
  \begin{prop}\label{derivadadeptiempo}
    Sea $ X:J\times M \rightarrow TM $ un campo dependiente del tiempo y $ \varphi:V\rightarrow M $ el flujo dependiente del tiempo asociado. Si $\alpha_t$ es una $r$-forma dependiente del tiempo, entonces para cualquier $(t_1,t_0,x)\in V$ 
   \begin{equation*}
     \left.\frac{d}{dt}\right\lvert _{t=t_1}\left(\varphi^*_{t,t_0} \alpha_t\right)_x = \left[\varphi^*_{t_1,t_0} \left( L_{X_{t_1}}\alpha_{t_1} + \left.\frac{d}{dt}\right\lvert_{t=t_1}\alpha_t\right) \right]_x.
   \end{equation*}
\end{prop}
   Podemos ver esta fórmula en una notación más compacta:

    \begin{equation*}
      \frac{d}{dt}\varphi^*_t \alpha_t = \varphi^*_t \left( L_{X_t}\alpha_t + \frac{d}{dt}\alpha_t \right).
    \end{equation*}

  \begin{proof}
    En primer lugar, probaremos una forma más sencilla, en la que $\alpha$ no depende del tiempo,
    \begin{equation*}
      \left.\frac{d}{dt}\right|_{t=t_1}(\varphi^*_{t,t_0}\alpha)=\varphi^*_{t_1,t_0}(L_{X_{t_1}}\alpha).
    \end{equation*}
    En efecto
    \begin{equation*}
      \left.\frac{d}{dt}\right|_{t=t_1}(\varphi_{t,t_0}\alpha)=\lim_{h\rightarrow 0}\frac{\varphi^*_{t_1+h,t_0}\alpha-\varphi^*_{t_1,t_0}\alpha}{h}=\varphi^*_{t_1,t_0}\left( \lim_{h\rightarrow 0 }\frac{\varphi_{t_1+h,t_1}\alpha-\alpha}{h} \right)=\varphi^*_{t_1,t_0}(L_{X_t}\alpha).
    \end{equation*}

    Probemos ahora la fórmula general. Tomando un $\varepsilon > 0$ lo suficientemente pequeño, consideremos la aplicación $F:(t_1-\varepsilon,t_1+\varepsilon)\times(t_1-\varepsilon,t_1+\varepsilon)\rightarrow\Gamma^k(T_xM)$ definida por
    \begin{equation*}
      F(u,v)=(\varphi^*_{u,t_0}\alpha_v)_x.
    \end{equation*}
    Ahora, por la regla de la cadena
    \begin{equation*}
      \left. \frac{d}{dt}\right|_{t=t_1}F(t,t)=\parcial{F}{u}(t_1,t_1)+\parcial{F}{v}(t_1,t_1)=\left[\varphi^*_{t_1,t_0}(L_{X_{t_1}}\alpha_{t_1})\right]_x+\left. \frac{d}{dt}\right|_{t=t_1}(\varphi^*_{t_1,t_0}\alpha_t)_x.
    \end{equation*}
    «Sacando factor común» a $\varphi^*_{t_1,t_0}$, obtenemos lo que queríamos probar.
  \end{proof}
  \section{Lema de Poincaré}
  En esta sección vamos a probar uno de los resultados fundamentales de cohomología de de Rham, que nos será útil en varias partes a lo largo del texto. Se trata del lema de Poincaré, que afirma que en un abierto \emph{estrellado} toda forma diferencial cerrada es exacta.

  Sea $M$ una variedad diferenciable de dimensión $n$ y sea $J$ el intervalo $[0,1]$. Consideremos la familia de aplicaciones
\begin{align*}
  j_t :M&\longrightarrow J\times M\\ 
    x &\longmapsto (t,x). 
  \end{align*}
  Se define el \emph{operador de homotopía} como la aplicación
  \begin{align*}
    h :\Gamma^k(J\times M)&\longrightarrow \Gamma^{k-1}(M)\\ 
    \omega &\longmapsto \int_{0}^1 i_{\frac{\partial}{\partial t}}\omega\ dt. 
    \end{align*}
    Explícitamente, para un punto $(t,x)\in J\times M$,
    \begin{equation*}
      (h\omega)_{x }(X_1,\dots,X_{k-1})=\int_{0}^1 (i_{\frac{\partial}{\partial t}}\omega)_{(t,x)}(X_1,\dots,X_{k-1}) dt=\int_{0}^1 \omega_{(t,x)}\left(\frac{\partial}{\partial t},X_1,\dots,X_{k-1}\right) dt.
    \end{equation*}

    \begin{center}
      \begin{tikzcd}
\vdots \arrow{d}{\mathrm{d}}&& \vdots	\arrow{d}{\mathrm{d}}\\ 
\Gamma^{k-1}(J\times U) \arrow{d}{\mathrm{d}} && \Gamma^{k-1}(U)\arrow{d}{\mathrm{d}}\\
\Gamma^{k}(J\times U)\arrow{rru}{h}\arrow{d}{\mathrm{d}} && \Gamma^{k}(U)\arrow{d}{\mathrm{d}}\\
\Gamma^{k+1}(J\times U)\arrow{rru}{h}\arrow{d}{\mathrm{d}} && \Gamma^{k+1}(U)\arrow{d}{\mathrm{d}}\\
\vdots && \vdots
\end{tikzcd}
    \end{center}

    \begin{prop}
      El operador de homotopía cumple la relación
      \begin{equation*}
	\mathrm{d} \circ h + h \circ \mathrm{d} = j_1^* - j_0^*,
      \end{equation*}
      con las $j_t$ definidas como antes.
    \end{prop}
    
    \begin{proof}
      Sea $\omega \in \Gamma^k(J\times M)$.
      Directamente, usando la fórmula de Cartan,
      \begin{align*}
	(\mathrm{d}\circ h + h\circ\mathrm{d})(\omega_{(t,x)})&= \mathrm{d}\left( \int_0^1 (i_{\frac{\partial}{\partial t}}\omega)_{(t,x)}dt \right)+ \int_0^1 i_{\frac{\partial}{\partial t}}(\mathrm{d}\omega_{(t,x)})dt\\
&	= \int_0^1 (\mathrm{d}\circ i_{\frac{\partial}{\partial t}} + i_{\frac{\partial}{\partial t}}\circ \mathrm{d})(\omega)_{(t,x)}dt\\
	&= \int_0^1 (L_{\frac{\partial}{\partial t}}\omega)_{(t,x)}dt
	= \int_0^1 \left(\frac{\partial}{\partial t} \omega\right)_{(t,x)}dt\\
	&	= \omega_{(1,x)}-\omega_{(0,x)}=\omega_{ j_1 (x)} - \omega_{ j_0(x)} \\ & = (j_1^* \omega)_x - (j_2^* \omega)_x.
      \end{align*}
    \end{proof}

    \begin{defn}
      \em
      Un subconjunto abierto $U\subset \mathbb{R}^n$ se dice \emph{estrellado} si, para cada $x\in U$ y para cada $t \in J$, $tx \in U$.
    \end{defn}
    Por ejemplo, son conjuntos estrellados las bolas centradas en $0$ o el propio $\mathbb{R} ^n$.
    Podemos considerar entonces el \emph{retracto de deformación}
    \begin{align*}
      r :J\times U&\longrightarrow U\\ 
        (t,x) &\longmapsto tx, 
      \end{align*}
      que es una aplicación diferenciable que cumple $r\circ j_0=r(0,\bullet)\equiv 0$ y $r\circ j_1 = r(1,\bullet)= \mathrm{id}_U$.

      \begin{thm}[Lema de Poincaré]
	Si $U\subset \mathbb{R}^n$ es un abierto estrellado, entonces toda forma diferencial cerrada en $U$ es exacta. En otras palabras, para todo $k\in \mathbb{N} $, $H^k(U)=0$.
      \end{thm}

      \begin{proof}
	Sea $\omega \in \Gamma^k(U)$. Sea $r$ el retracto de deformación y $\tilde{h}$ la aplicación inducida por el diagrama
	\begin{center}
	  \begin{tikzcd}
	    \Gamma^k(U)	    \arrow{r}{r^*}\arrow{rd}[anchor=north,rotate=-30]{\tilde{h}} & \Gamma^k(J\times U)\arrow{d}[anchor=west]{h} \\ 
	     &\Gamma^{k-1}(U).
	   \end{tikzcd}
	 \end{center}
	 Tenemos entonces que 
	 \begin{align*}
	   \tilde{h}(\mathrm{d}\omega)+\mathrm{d}(\tilde{h}\omega)&=h(r^*\mathrm{d}\omega)+\mathrm{d}(hr^*\omega)=h\circ \mathrm{d}(r^*\omega)+\mathrm{d}\circ h(r^*\omega)\\
	   &= j_1^*(r^*\omega)-j_0^*(r^*\omega)=(r\circ j_1)^*\omega - (r\circ j_0)^* \omega\\& = \omega - 0 = \omega.
	 \end{align*}
	 Ahora, si $\omega$ es cerrada, entonces $\mathrm{d}(\tilde{h}\omega)=\omega$, luego $\omega$ es exacta.
      \end{proof}

      \begin{corol}
	Para cualesquiera $n, k \in \mathbb{N} $, $H^k(\mathbb{R} ^n)=0$. Es decir, en $\mathbb{R}^n$ toda forma cerrada es exacta.
      \end{corol}

 \begin{corol}
   Si $n\geq 3$, entonces $H^2(\mathbb{S}^n)={0}$.
\end{corol}

\begin{proof}
  Si $\omega$ es una 2-forma en $\mathbb{S}^n$, $n\geq 3$, $x\in \mathbb{S}^n$ y $U$ es un disco entorno de $x$. Por el lema de Poincaré, existe una 1-forma $\alpha$ definida en $U$ tal que $\omega=\mathrm{d} \alpha$ en $U$. Sea $\theta$ una función meseta que valga 1 en un entorno $V\subset U$ de $x$ relativamente compacto de $U$ y $0$ fuera de $U$. La forma 
  \begin{equation*}
    \omega_1=\omega - \mathrm{d}(\theta \alpha)
  \end{equation*}
  es cerrada con soporte compacto en $\mathbb{S}^n \backslash \{x\}$, que es difeomorfa, por proyección estereográfica $\varphi$, a $\mathbb{R} ^n$. De nuevo por el lema de Poincaré, existe una 1-forma $\beta$ de $\mathbb{R} ^n$ tal que $\mathrm{d} \beta = \varphi^* \omega_1$. Con todo esto, tenemos
  \begin{equation*}
    \omega=\mathrm{d}\left(\theta \alpha + \left( \varphi^{-1} \right)^* \beta \right) .
  \end{equation*}
  Por tanto, $\omega$ es exacta en $\mathbb{S}^n$ y 
$    H^2(\mathbb{S}^n)= \{0\}$.
\end{proof}

